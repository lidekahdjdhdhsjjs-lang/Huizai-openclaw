#!/usr/bin/env python3
"""
ModelScope AIè§†é¢‘ç”Ÿæˆå™¨ - å®Œæ•´ç‰ˆ
ä½¿ç”¨I2VGen-XLå›¾ç”Ÿè§†é¢‘æ¨¡å‹
"""

import os
import time

# ModelScopeé…ç½®
MODELSCOPE_TOKEN = "ms-ecf935d5-75fe-4486-a2d2-876a39d5f3a1"

os.environ['MODELSCOPE_TOKEN'] = MODELSCOPE_TOKEN

from modelscope.hub.api import HubApi
from modelscope.pipelines import pipeline
from modelscope.outputs import OutputKeys

# è¾“å‡ºç›®å½•
OUTPUT_DIR = '/home/li/.openclaw/workspace/drama/video_gen/modelscope'
os.makedirs(OUTPUT_DIR, exist_ok=True)

# å…¨å±€pipeline (é¿å…é‡å¤åŠ è½½)
_video_pipeline = None

def get_video_pipeline():
    """è·å–è§†é¢‘ç”Ÿæˆpipeline"""
    global _video_pipeline
    
    if _video_pipeline is None:
        print("ğŸ”„ åŠ è½½I2VGen-XLæ¨¡å‹...")
        try:
            # I2VGen-XL: å›¾ç”Ÿè§†é¢‘
            _video_pipeline = pipeline(
                'image-to-video', 
                model='i2vgen-xl',
                model_revision='v1.0'
            )
            print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ!")
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            # ä½¿ç”¨å¤‡é€‰
            _video_pipeline = None
    
    return _video_pipeline

def generate_image_to_video(image_path, prompt="", output_name="output"):
    """
    å›¾ç”Ÿè§†é¢‘ (I2VGen-XL)
    
    Args:
        image_path: è¾“å…¥å›¾ç‰‡è·¯å¾„
        prompt: æç¤ºè¯
        output_name: è¾“å‡ºæ–‡ä»¶å
    """
    
    pipeline = get_video_pipeline()
    
    if pipeline is None:
        return generate_fallback_video(image_path, prompt, output_name)
    
    print(f"ğŸ¬ æ­£åœ¨ç”Ÿæˆè§†é¢‘...")
    print(f"   å›¾ç‰‡: {image_path}")
    print(f"   æç¤ºè¯: {prompt}")
    
    try:
        # è°ƒç”¨ModelScope API
        output = pipeline({
            'image': image_path,
            'prompt': prompt
        })
        
        # ä¿å­˜è§†é¢‘
        output_path = f"{OUTPUT_DIR}/{output_name}_{int(time.time())}.mp4"
        
        # æ£€æŸ¥è¾“å‡º
        if OutputKeys.OUTPUT_VIDEO in output:
            with open(output_path, 'wb') as f:
                f.write(output[OutputKeys.OUTPUT_VIDEO])
            print(f"âœ… è§†é¢‘ç”ŸæˆæˆåŠŸ: {output_path}")
        else:
            print(f"âš ï¸ æœªç”Ÿæˆè§†é¢‘ï¼Œä½¿ç”¨å¤‡é€‰æ–¹æ¡ˆ")
            return generate_fallback_video(image_path, prompt, output_name)
        
        return output_path
        
    except Exception as e:
        print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
        return generate_fallback_video(image_path, prompt, output_name)

def generate_text_to_image(prompt, output_name="image"):
    """
    æ–‡ç”Ÿå›¾ (Stable Diffusion)
    """
    
    print(f"ğŸ–¼ï¸ æ­£åœ¨ç”Ÿæˆå›¾ç‰‡: {prompt}")
    
    try:
        # ä½¿ç”¨SDæ¨¡å‹
        sd_pipeline = pipeline(
            'text-to-image', 
            model='stable-diffusion-v1.5',
            model_revision='v1.0'
        )
        
        output = sd_pipeline({
            'text': prompt,
            'num_inference_steps': 20
        })
        
        # ä¿å­˜å›¾ç‰‡
        output_path = f"{OUTPUT_DIR}/{output_name}_{int(time.time())}.jpg"
        output[OutputKeys.OUTPUT_IMAGE].save(output_path)
        
        print(f"âœ… å›¾ç‰‡ç”ŸæˆæˆåŠŸ: {output_path}")
        return output_path
        
    except Exception as e:
        print(f"âŒ å›¾ç‰‡ç”Ÿæˆå¤±è´¥: {e}")
        # åˆ›å»ºå ä½å›¾
        from PIL import Image, ImageDraw, ImageFont
        
        img = Image.new('RGB', (720, 1280), color=(30, 30, 60))
        draw = ImageDraw.Draw(img)
        
        try:
            font = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf", 36)
        except:
            font = ImageFont.load_default()
        
        draw.text((360, 640), prompt[:20], fill=(255, 255, 255), font=font, anchor="mm")
        
        output_path = f"{OUTPUT_DIR}/{output_name}_{int(time.time())}.jpg"
        img.save(output_path)
        
        return output_path

def generate_fallback_video(image_path, prompt, output_name):
    """ä½¿ç”¨ffmpegç”Ÿæˆå¤‡é€‰è§†é¢‘"""
    
    video_path = f"{OUTPUT_DIR}/{output_name}_{int(time.time())}.mp4"
    
    import subprocess
    subprocess.run([
        'ffmpeg', '-y',
        '-loop', '1', '-i', image_path,
        '-c:v', 'libx264', '-t', '4',
        '-vf', 'zoompan=z=1.05:d=4:s=720x1280',
        '-shortest', video_path
    ], capture_output=True)
    
    print(f"âœ… å¤‡é€‰è§†é¢‘ç”Ÿæˆ: {video_path}")
    return video_path

def full_workflow(prompt, output_name="drama"):
    """
    å®Œæ•´å·¥ä½œæµ: æ–‡ç”Ÿå›¾ â†’ å›¾ç”Ÿè§†é¢‘
    """
    
    print(f"\n=== å®Œæ•´ç”Ÿæˆå·¥ä½œæµ ===")
    print(f"ä¸»é¢˜: {prompt}\n")
    
    # 1. æ–‡ç”Ÿå›¾
    print("1ï¸âƒ£ æ­¥éª¤1: ç”Ÿæˆå›¾ç‰‡...")
    image_path = generate_text_to_image(prompt, output_name)
    
    # 2. å›¾ç”Ÿè§†é¢‘
    print("\n2ï¸âƒ£ æ­¥éª¤2: ç”Ÿæˆè§†é¢‘...")
    video_path = generate_image_to_video(image_path, prompt, output_name)
    
    print(f"\nâœ… å®Œæˆ! è§†é¢‘: {video_path}")
    return video_path

if __name__ == '__main__':
    print("=== ModelScope AIè§†é¢‘ç”Ÿæˆå™¨ ===\n")
    
    # æµ‹è¯•ç”Ÿæˆ
    video = full_workflow("A handsome man in business suit standing in front of skyscraper", "test")
    print(f"\nğŸ“ è¾“å‡º: {video}")
