#!/usr/bin/env python3
"""
AIè§†é¢‘ç”Ÿæˆå™¨ - åŸºäºHugging Faceå…è´¹API
ä½¿ç”¨å¼€æºæ¨¡å‹ç”Ÿæˆè§†é¢‘ (æ— éœ€GPU)
"""

import os
import time
import json
import requests
from datetime import datetime

# é…ç½®
HF_TOKEN = os.environ.get('HF_TOKEN', '')  # Hugging Face Token
API_URL = "https://api-inference.huggingface.co/models/stabilityai/stable-video-diffusion"

# è¾“å‡ºç›®å½•
OUTPUT_DIR = '/home/li/.openclaw/workspace/drama/video_gen'
os.makedirs(OUTPUT_DIR, exist_ok=True)

def generate_video_from_image(image_path, prompt="", duration=2):
    """
    ä½¿ç”¨Stable Video Diffusionä»å›¾ç‰‡ç”Ÿæˆè§†é¢‘
    
    æ³¨æ„: éœ€è¦Hugging Face Proè´¦æˆ·æ‰èƒ½ä½¿ç”¨SVD
    å…è´¹æ–¹æ¡ˆä½¿ç”¨æ›¿ä»£æ–¹æ¡ˆ
    """
    
    if not HF_TOKEN:
        print("âš ï¸ æœªè®¾ç½®HF_TOKENï¼Œä½¿ç”¨å¤‡é€‰æ–¹æ¡ˆ")
        return generate_video_placeholder(image_path, prompt)
    
    headers = {"Authorization": f"Bearer {HF_TOKEN}"}
    
    with open(image_path, "rb") as f:
        data = f.read()
    
    response = requests.post(API_URL, headers=headers, data=data)
    
    if response.status_code == 200:
        output_path = f"{OUTPUT_DIR}/video_{int(time.time())}.mp4"
        with open(output_path, "wb") as f:
            f.write(response.content)
        return output_path
    else:
        print(f"âŒ APIé”™è¯¯: {response.status_code}")
        return generate_video_placeholder(image_path, prompt)

def generate_video_placeholder(image_path, prompt):
    """ç”Ÿæˆå ä½è§†é¢‘ï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
    import subprocess
    
    output_path = f"{OUTPUT_DIR}/video_{int(time.time())}.mp4"
    
    # ä½¿ç”¨ffmpegä»å›¾ç‰‡ç”ŸæˆçŸ­è§†é¢‘ï¼ˆå¸¦ç®€å•åŠ¨ç”»ï¼‰
    subprocess.run([
        'ffmpeg', '-y',
        '-loop', '1', '-i', image_path,
        '-c:v', 'libx264', '-t', '4',
        '-vf', 'zoompan=z=1.2:d=4:s=720x1280',
        '-shortest', output_path
    ], capture_output=True)
    
    return output_path

def generate_from_text(prompt, style="cinematic"):
    """
    æ–‡æœ¬ç”Ÿæˆè§†é¢‘ - ä½¿ç”¨Zeroscope (å…è´¹æ–¹æ¡ˆ)
    """
    
    # Zeroscopeæ˜¯ä¸€ä¸ªå…è´¹çš„æ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹
    # å¯ä»¥é€šè¿‡Replicate APIè°ƒç”¨ï¼ˆéœ€è¦API Keyï¼‰
    
    print(f"ğŸ“¹ æ–‡æœ¬ç”Ÿæˆè§†é¢‘: {prompt}")
    
    # å¤‡é€‰æ–¹æ¡ˆï¼šç”Ÿæˆä¸€å¼ å›¾ç„¶åè½¬è§†é¢‘
    from PIL import Image, ImageDraw, ImageFont
    
    # åˆ›å»ºå›¾ç‰‡
    img = Image.new('RGB', (720, 1280), color=(20, 20, 40))
    draw = ImageDraw.Draw(img)
    
    # æ·»åŠ æ–‡å­—
    try:
        font = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf", 32)
    except:
        font = ImageFont.load_default()
    
    # åˆ†è¡Œæ˜¾ç¤º
    words = prompt.split()
    lines = []
    current_line = ""
    for word in words:
        if len(current_line + " " + word) < 30:
            current_line += " " + word if current_line else word
        else:
            lines.append(current_line)
            current_line = word
    lines.append(current_line)
    
    y = 500
    for line in lines:
        draw.text((360, y), line, fill=(255, 255, 255), font=font, anchor="mm")
        y += 50
    
    img_path = f"{OUTPUT_DIR}/temp_{int(time.time())}.jpg"
    img.save(img_path)
    
    # è½¬è§†é¢‘
    video_path = generate_video_placeholder(img_path, prompt)
    
    return video_path

def check_available_models():
    """æ£€æŸ¥å¯ç”¨çš„å…è´¹è§†é¢‘ç”Ÿæˆæ–¹æ¡ˆ"""
    
    print("\n=== å¯ç”¨æ–¹æ¡ˆ ===")
    print("1. Hugging Face SVD (éœ€è¦Proè´¦æˆ·)")
    print("2. Replicate (éœ€è¦API Key)")
    print("3. RunwayML (éœ€è¦å®‰è£…)")
    print("4. æœ¬åœ°éƒ¨ç½² SVD (éœ€è¦NVIDIA GPU)")
    print("5. å…è´¹å›¾ç”Ÿè§†é¢‘ (æœ¬æ–‡æ¡£æ–¹æ¡ˆ)")
    print("")
    
    return [
        {"name": "Zeroscope", "api": "Replicate", "cost": "$0.01/ç§’"},
        {"name": "ModelScope", "api": "å…è´¹", "cost": "å…è´¹"},
        {"name": "SVD", "api": "Hugging Face", "cost": "ä»˜è´¹"},
    ]

# ============ æ¨èæ–¹æ¡ˆ ============

def setup_free_video_api():
    """è®¾ç½®å…è´¹è§†é¢‘API"""
    
    config = """
# å…è´¹AIè§†é¢‘ç”Ÿæˆæ–¹æ¡ˆé…ç½®

## æ–¹æ¡ˆ1: ModelScope (å…è´¹å›½å†…)
- API: https://modelscope.cn
- æ¨¡å‹: I2VGen-XL
- è´¹ç”¨: å…è´¹
- æ¥å…¥: éœ€è¦æ³¨å†ŒModelScope

## æ–¹æ¡ˆ2: Replicate (ç¨³å®š)
- API: https://replicate.com
- æ¨¡å‹: zeroscope_v2
- è´¹ç”¨: $0.01/ç§’
- æ¥å…¥: éœ€è¦æ³¨å†Œå¹¶è·å–API Key

## æ–¹æ¡ˆ3: Hugging Face (SVD)
- API: https://huggingface.co
- æ¨¡å‹: stable-video-diffusion
- è´¹ç”¨: éœ€è¦Proè´¦æˆ·
- æ¥å…¥: éœ€è¦ç”³è¯·

## å¿«é€Ÿå¼€å§‹ (Replicate)

1. æ³¨å†Œ https://replicate.com
2. è·å–API Token
3. å®‰è£…: pip install replicate
4. ä½¿ç”¨ç¤ºä¾‹:

import replicate
output = replicate.run(
    "zeroscope/v2-576w:ugriiIYNYFFrD8NDDJZGMF6gYY4fMq4KkPfZ8CmL2NDi",
    input={"prompt": "a person walking in rain"}
)
"""
    
    print(config)
    return config

if __name__ == '__main__':
    print("=== AIè§†é¢‘ç”Ÿæˆå™¨ ===\n")
    
    # æ£€æŸ¥å¯ç”¨æ–¹æ¡ˆ
    check_available_models()
    
    # è®¾ç½®æŒ‡å—
    setup_free_video_api()
    
    # æµ‹è¯•ç”Ÿæˆ
    print("\n=== æµ‹è¯•ç”Ÿæˆ ===")
    test_video = generate_from_text("A handsome man in suit walking in city")
    print(f"âœ… æµ‹è¯•è§†é¢‘: {test_video}")
